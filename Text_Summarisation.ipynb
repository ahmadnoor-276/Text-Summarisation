{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo32yy8vB1Wi"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTXUnOVJykIh",
        "outputId": "bfa855cc-8884-44cd-db93-5a94c538087e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary modules from the Hugging Face Transformers library\n",
        "from transformers import PegasusForConditionalGeneration\n",
        "from transformers import PegasusTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "#Model for summarisation\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "\n",
        "# Load pretrained tokenizer\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "7QNHXk88t6SX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_summarise = \"\"\"Recurrent neural networks leverage backpropagation through time (BPTT) algorithm to determine the gradients, which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. These calculations allow us to adjust and fit the parameters of the model appropriately. BPTT differs from the traditional approach in that BPTT sums errors at each time step whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.\n",
        "                    Through this process, RNNs tend to run into two problems, known as exploding gradients and vanishing gradients. These issues are defined by the size of the gradient, which is the slope of the loss function along the error curve. When the gradient is too small, it continues to become smaller, updating the weight parameters until they become insignificantâ€”i.e. 0. When that occurs, the algorithm is no longer learning. Exploding gradients occur when the gradient is too large, creating an unstable model. In this case, the model weights will grow too large, and they will eventually be represented as NaN. One solution to these issues is to reduce the number of hidden layers within the neural network, eliminating some of the complexity in the RNN model.\"\"\""
      ],
      "metadata": {
        "id": "svdFWRYH1sHk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing PEGASUS model\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "#Tokenizes the input text using the PEGASUS tokenizer\n",
        "tokens = pegasus_tokenizer(text_to_summarise, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "zueFfErqvJ3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating summary based on the tokenized input.\n",
        "encoded_summary = pegasus_model.generate(**tokens)"
      ],
      "metadata": {
        "id": "n8x8MUdj0ALY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding summarized text\n",
        "decoded_summary = pegasus_tokenizer.decode(\n",
        "      encoded_summary[0],\n",
        "      skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "4emoMlz50JmF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define summarization pipeline\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model_name,\n",
        "    tokenizer=pegasus_tokenizer,\n",
        "    framework=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "5kKZ7uN10QhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Topic Summary\n",
        "summary = summarizer(text_to_summarise, min_length=50, max_length=120)"
      ],
      "metadata": {
        "id": "iY_Mm5xe0Yz5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary[0][\"summary_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "J_nb087r0uE_",
        "outputId": "6bbb696b-b6e5-4b13-a659-4851baeac6f4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recurrent neural networks (RNNs) train themselves by learning a gradient, which is the slope of the loss function along the error curve, which is the input layer of an RNN and the output layer of a feedforward neural network.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}